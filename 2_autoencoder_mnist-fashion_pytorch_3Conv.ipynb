{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_autoencoder_mnist-fashion_pytorch_3Conv.ipynb","provenance":[{"file_id":"1MLphkWbZfR7OcfPWpyBRs7AS9bmJXoBR","timestamp":1572954245872}],"collapsed_sections":[]},"kernelspec":{"name":"python_defaultSpec_1598713657435","display_name":"Python 3.7.1 64-bit ('base': conda)"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ul9ib40gDEsB","colab_type":"code","outputId":"5a3d861a-0e74-4c4e-a064-5d638d5bb132","executionInfo":{"status":"ok","timestamp":1571902869828,"user_tz":-360,"elapsed":171766,"user":{"displayName":"Hasnain Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAwY6zFkaan1YVFlYYb_yeNxUypN6jZ3QOwKAkGVvY=s64","userId":"15648618347506191120"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"source":["import torch\n","import numpy as np\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","from torch import nn\n","import os\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","if not os.path.exists('./dc_img'):\n","    os.mkdir('./dc_img')\n","\n","\n","def to_img(x):\n","    x = 0.5 * (x + 1)\n","    x = x.clamp(0, 1)\n","    x = x.view(x.size(0), 1, 28, 28)\n","    return x\n","\n","\n","num_epochs = 10\n","batch_size = 128\n","learning_rate = 1e-3\n","\n","img_transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.5,), (0.5,))])\n","\n","dataset = datasets.FashionMNIST('~/.pytorch/MNIST_Fashion_data/', download=True, transform=img_transform)\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        # encoder\n","        self.conv1 = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n","        self.pool1 = nn.MaxPool2d(2, stride=2)\n","\n","        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n","        self.pool2 = nn.MaxPool2d(2, stride=2)\n","        \n","        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n","        self.pool3 = nn.MaxPool2d(2, stride=2, padding=1)\n","\n","        # decoder\n","        self.decoder1 = nn.ConvTranspose2d(128, 64, 3, stride=2)\n","        self.decoder2 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=2)\n","        self.decoder3 = nn.ConvTranspose2d(32, 1, 2, stride=2, padding=1)\n","\n","    def forward(self, x):\n","\n","        # add sequence of convolutional and max pooling layers\n","        x = self.pool1(F.relu(self.conv1(x)))\n","        x = self.pool2(F.relu(self.conv2(x)))\n","        x = self.pool3(F.relu(self.conv3(x)))\n","\n","        x = F.relu(self.decoder1(x))\n","        x = F.relu(self.decoder2(x))\n","        x = F.tanh(self.decoder3(x))  \n","        return x\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","criterion = nn.MSELoss()\n","model = Net()\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n","                             weight_decay=1e-5)\n","\n","for epoch in range(num_epochs):\n","    for data in dataloader:\n","        img, _ = data\n","        img = Variable(img).cuda()\n","        # ===================forward=====================\n","        output = model(img)\n","        loss = criterion(output, img)\n","        # ===================backward====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # ===================log========================\n","    print('epoch [{}/{}], loss:{:.4f}'\n","          .format(epoch+1, num_epochs, loss.data))\n","    if epoch % 10 == 0:\n","        pic = to_img(output.cpu().data)\n","        save_image(pic, './dc_img/image_{}.png'.format(epoch))\n","\n","torch.save(model.state_dict(), './conv_autoencoder.pth')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  0%|          | 98304/26421880 [00:00<00:28, 907795.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["26427392it [00:00, 82058846.90it/s]                             \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 588288.73it/s]\n","4423680it [00:00, 32263226.14it/s]                           "],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","Extracting /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["\n","8192it [00:00, 189002.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","Extracting /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_Fashion_data/FashionMNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["epoch [1/10], loss:0.0386\n","epoch [2/10], loss:0.0286\n","epoch [3/10], loss:0.0240\n","epoch [4/10], loss:0.0232\n","epoch [5/10], loss:0.0198\n","epoch [6/10], loss:0.0227\n","epoch [7/10], loss:0.0168\n","epoch [8/10], loss:0.0158\n","epoch [9/10], loss:0.0147\n","epoch [10/10], loss:0.0153\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ovswVqcuE-gA","colab_type":"code","outputId":"c838a60c-588b-4627-cb6c-91572a9bbb12","executionInfo":{"status":"ok","timestamp":1570651141807,"user_tz":-360,"elapsed":2410,"user":{"displayName":"Tahmid Bin Mahmud","photoUrl":"","userId":"12031929725368845653"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["model = Net()\n","model.load_state_dict(torch.load('./conv_autoencoder.pth'))\n","model.eval()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (decoder1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n","  (decoder2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n","  (decoder3): ConvTranspose2d(32, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",")"]},"metadata":{"tags":[]},"execution_count":17}]}]}