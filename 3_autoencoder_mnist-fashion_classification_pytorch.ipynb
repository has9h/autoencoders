{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_autoencoder_mnist-fashion_classification_pytorch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"30HODAjg0P33","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","import numpy as np\n","from PIL import Image\n","import copy\n","import time\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D8qNslmC3NpH","colab_type":"text"},"source":["### Downloading Dataset"]},{"cell_type":"code","metadata":{"id":"0-0FQCYf3Jzi","colab_type":"code","colab":{}},"source":["trainLoader = torch.utils.data.DataLoader(\n","    datasets.FashionMNIST('./fmnist/', train=True, download=True, \n","                          transform=transforms.Compose([\n","                                transforms.ToTensor()\n","                          ])), batch_size=1024, shuffle=True, num_workers=1 ######, pin_memory...\n",")\n","\n","testLoader = torch.utils.data.DataLoader(\n","    datasets.FashionMNIST('./fmnist/', train=False,\n","                          transform=transforms.Compose([\n","                              transforms.ToTensor()\n","                          ])), batch_size=1024, shuffle=True, num_worders=1 ######, pin_memory...\n",")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4S4JAMIq3f2K","colab_type":"code","colab":{}},"source":["# Size of train and test datasets\n","print(\"Number of samples in train set: \" + str(len(trainLoader.dataset)))\n","print(\"Number of samples in test set: \" + str(len(testLoader.dataset)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXKMowoV6Opt","colab_type":"code","colab":{}},"source":["# Sample image from the dataset\n","img = trainLoader.dataset[0][0]\n","img_np = img.squeeze(0).numpy()\n","plt.imshow(img_np, cmap='gray')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0TZ87aY66cBQ","colab_type":"code","colab":{}},"source":["use_gpu = torch.cuda.is_available()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-So1d81b6ky0","colab_type":"text"},"source":["### Define the Autoencoder class"]},{"cell_type":"code","metadata":{"id":"Ig7ohTKY6pSb","colab_type":"code","colab":{}},"source":["class autoencoder(nn.Module):\n","    def __init__(self):\n","        super(autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(28*28, 100),\n","            nn.ReLU(),\n","            nn.Linear(100, 100),\n","            nn.ReLU())\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(100, 100),\n","            nn.ReLU(),\n","            nn.Linear(100, 28*28),\n","            nn.ReLU()\n","        )\n","\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"REW3KhfL71lm","colab_type":"text"},"source":["### Defining function for training the network for reconstruction"]},{"cell_type":"code","metadata":{"id":"fHYNqWw77uyR","colab_type":"code","colab":{}},"source":["# Defining the training routine\n","def train_model_recon(model, criterion, optimizer, num_epochs):\n","    start = time.time()\n","    # List for saving the loss per epoch:\n","    train_loss = []\n","    \n","    for epoch in range(num_epochs):\n","        epochStartTime = time.time()\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","\n","        running_loss = 0.0\n","        # Loading data in batches:\n","        batch = 0\n","        \n","        for data in trainLoader:\n","            inputs, labels = data\n","            # Wrap them in Variable:\n","            if use_gpu:\n","                inputs, labels = Variable(inputs.view(inputs.size(0),-1).cuda)#########\n","            else:\n","                inputs, labels = Variable(inputs.view(inputs.size(0),-1))######, Variable\n","            # Initializing model gradients to zero:\n","            mode.zero_grad()\n","            # Data feed-forward through the network\n","            outputs = model(inputs)\n","            # Finding the MSE:\n","            loss = criterion(outputs, labels)\n","\n","            # Backpropagating the loss and updating the model parameters:\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Accumulating the loss for each batch:\n","            running_loss += loss.data[0]\n","            if batch == 0:\n","                totalLoss = loss\n","                batch += 1\n","            else:\n","                totalLss += loss\n","                batch += 1\n","        \n","        # Total loss for one epoch\n","        epoch_loss = running_loss/50000\n","        # Saving the loss over epochs for plotted\n","        train_loss.append(epoch_loss) \n","\n","        print('Epoch loss: {:.6f}'.format(epoch_loss))\n","        epochTimeEnd = time.time() - epochStartTime\n","        print('Epoch complete in {:.0f}m {:.0f}s'.format(\n","            epochTimeEnd // 60, epochTimeEnd % 60\n","        ))\n","        print('-' * 25)\n","\n","        # Plotting Loss vs Epochs\n","        fig1 = plt.figure(1)\n","        plt.plot(range(epoch + 1), train_loss, 'r--', label='train')\n","\n","        if epoch == 0:\n","            plt.legend(loc='upper left')\n","            plt.xlabel('Epochs')\n","            plt.ylabel('Loss')\n","        fig1.savefig('aeRecon_lossPlot.png')\n","\n","    time_elapsed = time.time() - start\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60)\n","    )\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVRy3z2EMI6n","colab_type":"text"},"source":["### Model Initialization and Training"]},{"cell_type":"code","metadata":{"id":"2mWdRcG1MK_U","colab_type":"code","colab":{}},"source":["net = autoencoder()\n","print(net)\n","if use_gpu:\n","    net = net.cuda()\n","init_weights = copy.deepcopy(net.encoder[0].weight.data)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCOHWAG_MTaa","colab_type":"code","colab":{}},"source":["criterion = nn.MSELoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.5, momentum=0.9)\n","# Training the model:\n","net = train_model_recon(net, criterion, optimizer, num_epochs=2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SEla8rswMm7K","colab_type":"text"},"source":["### Reconstruction using trained Autoencoder"]},{"cell_type":"code","metadata":{"id":"DbP9sHSnMqXx","colab_type":"code","colab":{}},"source":["TestImg = testLoader.dataset[0][0]\n","\n","if use_gpu:\n","    outputImg = net(Variable(TestImg.view(TestImg.size(0), -1)).cuda())\n","else:\n","    outputImg = net(Variable(TestImg.view(TestImg.size(0), -1)))\n","\n","outputImg = outputImg.data.view(-1, 28, 28).cpu()\n","\n","fig = plt.figure()\n","plot = fig.add_subplot(1, 2, 1)\n","img = np.array(TestImg.numpy())[0]\n","plot.set_title('Reconstructed Image')\n","imgplot = plt.imshow(img, cmap='gray')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3UjBgNDDNmZ2","colab_type":"text"},"source":["### Visualization of weights of encoder"]},{"cell_type":"code","metadata":{"id":"zRAUwkFLNoxQ","colab_type":"code","colab":{}},"source":["trained_weights = copy.deepcopy(net.encoder[0].weight.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8sMWzxKlNuTV","colab_type":"code","colab":{}},"source":["init_weights = (1 + init_weights) * 127.5\n","trained_weights = (1 + trained_weights) * 127.5\n","\n","if use_gpu:\n","    init_weights = init_weights.view(-1, 280, 280).byte().cpu()\n","    trained_weights = trained_weights.view(-1, 280, 280).byte().cpu()\n","else:\n","    init_weights = init_weights.view(-1, 280, 280).byte()\n","    trained_weights = trained_weights.view(-1, 280, 280).byte()\n","\n","d_weights = init_weights - trained_weights\n","\n","fig = plt.figure()\n","plot = fig.add_subplot(1, 3, 1)\n","img = np.array(init_weights.numpy())[0]\n","plot.set_title('Initial Weights')\n","imgplot = plt.imshow(img, cmap='gray')\n","\n","plot = fig.add_subplot(1, 3, 2)\n","img = np.array(trained_weights.numpy())[0]\n","plot.set_title('Trained Weights')\n","imgplot = plt.imshow(img, cmap='gray')\n","\n","plot = fig.add_subplot(1, 3, 3)\n","img = np.array(d_weights.numpy())[0]\n","plot.set_title('Weight Update')\n","imgplot = plt.imshow(img, cmap='gray')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Gdl2dJzO-5D","colab_type":"text"},"source":["### Autoencoder for Classification"]},{"cell_type":"code","metadata":{"id":"nZ88mh9pPND8","colab_type":"code","colab":{}},"source":["new_classifier = nn.Sequential(*list(net.children())[:-1])\n","net = new_classifier\n","net.add_module('classifier', nn.Sequential(nn.Linear(100, 10), nn.LogSoftmax()))\n","print(net)\n","\n","if use_gpu:\n","    net = net.cuda()\n","cll_weights = copy.deepcopy(net[0][0].weight.data)\n","init_classifier_weights = copy.deepcopy(net.classifier[0].weight.data)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSBwveBQQn-0","colab_type":"text"},"source":["### Defining functions for training the network for classification"]},{"cell_type":"code","metadata":{"id":"8iabnub_Qq-L","colab_type":"code","colab":{}},"source":["# Defining the training routine\n","def train_model_clasif(model, criterion, optimizer, num_epochs):\n","    start = time.time()\n","    # List for saving the loss per epoch\n","    train_loss = []\n","    # List for saving the accuracy per epoch\n","    train_acc = []\n","\n","    for epoch in range(num_epochs):\n","        epochStartTime = time.time()\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","\n","        running_loss = 0.0\n","        running_corrects = 0.0\n","        # Loading data in batches\n","        batch = 0\n","        for data in trainLoader:\n","            inputs, labels = data\n","            # Wrap them in Variable:\n","            if use_gpu:\n","                inputs, labels = Variable(inputs.view(inputs.size(0), -1).cuda) ##########\n","                    Variable(labels.cuda())\n","            else:\n","                inputs, labels = Variable(inputs.view(inputs.size(0), -1))########, Variable\n","            # Initializing model gradients to zero\n","            model.zero_grad()\n","            # Data feed-forward through the network:\n","            outputs = model(inputs)\n","            # Finding the MSE\n","            loss = criterion(ouputs, labels)\n","\n","            # Backpropagating the loss and updating the model parameters\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Accumulating the loss for each batch\n","            running_loss += loss.data[0]\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Accumulating correct predictions for each batch\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","        # Average loss for one epoch:\n","        epoch_loss = running_loss/60000.0\n","        # Saving the loss over epochs for plotting:\n","        train_loss.append(epoch_loss) \n","        # Average accuracy for\n","        epoch_acc = float(running_corrects) / 60000.0\n","        # Saving the accuracy over\n","        train_acc.append(epoch_acc)\n","\n","        print('Epoch loss: {:.6f}, Epoch accuracy: {:.6f}'.format())\n","        epochTimeEnd = time.time() - epochStartTime\n","        print('Epoch complete in {:.0f}m {:.0f}s'.format(\n","            epochTimeEnd // 60, epochTimeEnd % 60\n","        ))\n","        print('-' * 25)\n","        \n","        ####################### Codes left out here:\n","        # fig1 = plt.figure(1)\n","        # plt.plot(range(epoch + 1), train_loss, 'r--', label='train')\n","\n","        # if epoch == 0:\n","        #     plt.legend(loc='upper left')\n","        #     plt.xlabel('Epochs')\n","        #     plt.ylabel('Accuracy')\n","        # fig1.savefig('aeClassif_accPlot.png')\n","\n","    time_elapsed = time.time() - start\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60)\n","    )\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wR4nGSMJtikW","colab_type":"text"},"source":["### Defining loss function and training the network"]},{"cell_type":"code","metadata":{"id":"yQGHYT8aX0TI","colab_type":"code","colab":{}},"source":["criterion = nn.NLLLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n","net = train_model_clasif(net, criterion, optimizer, num_epochs=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3L7f74DvPEG","colab_type":"text"},"source":["### Performance evaluation of trained network"]},{"cell_type":"code","metadata":{"id":"9c3zVp6QwUxE","colab_type":"code","colab":{}},"source":["# Finding testing accuracy\n","test_running_corr = 0\n","# Loading data in batches\n","batches = 0\n","running_corrects = 0.0\n","for tsData in testLoader:\n","    inputs, labels = tsData\n","    # Wrap them in Variable\n","    if use_gpu:\n","        inputs, labels = Variable(inputs.view(inputs.size(0), -1).cuda()), ######## Variable\n","    else:\n","        inputs, labels = Variable(inputs.view(inputs.size(0), -1)), ######Variable(label)\n","    # Feedforward train data batch through model:\n","    output = net(inputs)\n","    # Predicted class is the one with maximum probability\n","    _, preds = output.data.max(1)\n","    running_corrects += torch.sum(preds == labels.data)\n","\n","# Finding total number of correct predictions\n","ts_acc = running_corrects/10000.0\n","print('Accuracy on test set = ' + str(ts_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CvyF4y1gw8gL","colab_type":"text"},"source":["### Visualization of weights of encoder"]},{"cell_type":"code","metadata":{"id":"bdokGnAgw-jf","colab_type":"code","colab":{}},"source":["cll_weights_ft = copy.deepcopy(net[0][0].weight.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJUk36SsxEaq","colab_type":"code","colab":{}},"source":["cll_weights = (1 + cll_weights) * 127.5\n","cll_weights_ft = (1 + cll_weights_ft) * 127.5\n","\n","if use_gpu:\n","    cll_weights = cll_weights.view(-1, 280, 280).byte().cpu()\n","    cll_weights_ft = cll_weights_ft.view(-1, 280, 280).byte().cpu()\n","else:\n","    cll_weights = cll_weights.view(-1, 280, 280).byte()\n","    cll_weights_ft = cll_weights_ft.view(-1, 280, 280).byte()\n","\n","d_weights = cll_weights - cll_weights_ft\n","\n","fig = plt.figure()\n","plot = fig.add_subplot(1, 3, 1)\n","img = np.array(cll_weights.numpy())[0]\n","plot.set_title('Encoder Weights')\n","imgplot = plt.imshow(img, cmap='gray')\n","\n","plot = fig.add_subplot(1, 3, 2)\n","img = np.array(cll_weights_ft.numpy())[0]\n","plot.set_title('Finetuned Weights')\n","imgplot = plt.imshow(img, cmap='gray')\n","\n","plot = fig.add_subplot(1, 3, 3)\n","img = np.array(d_weights.numpy())[0]\n","plot.set_title('Weight Update')\n","imgplot = plt.imshow(img, cmap='gray')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HfH3dbTWyLNC","colab_type":"text"},"source":["### Variation of weights of Classifier"]},{"cell_type":"code","metadata":{"id":"OQF5FaGcyOCo","colab_type":"code","colab":{}},"source":["trained_classifier_weights = copy.deepcopy(net.classifier[0].weight.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rO-S28PX4ICY","colab_type":"code","colab":{}},"source":["init_classifier_weights = (1 + init_classifier_weights) * 255\n","trained_classifier_weights = (1 + trained_classifier_weights) * 255\n","\n","if use_gpu:\n","    init_classifier_weights = init_classifier_weights.view(-1, 40, 25).byte().cpu()\n","    trained_classifier_weights = trained_classifier_weights.view(-1, 40, 25).byte().cpu()\n","else:\n","    init_classifier_weights = init_classifier_weights.view(-1, 40, 25).byte()\n","    trained_classifier_weights = trained_classifier_weights.view(-1, 40, 25).byte()\n","\n","d_weights = init_classifier_weights - trained_classifier_weights\n","\n","fig = plt.figure()\n","plot = fig.add_subplot(1, 3, 1)\n","img = np.array(init_classifier_weights.numpy())[0]\n","plot.set_title('Initial Weights')\n","imgplot = plt.imshow(img, cmap='gray')\n","\n","fig = plt.figure()\n","plot = fig.add_subplot(1, 3, 1)\n","img = np.array(trained_classifier_weights.numpy())[0]\n","plot.set_title('Trained Weights')\n","imgplot = plt.imshow(img, cmap='gray')\n","\n","fig = plt.figure()\n","plot = fig.add_subplot(1, 3, 1)\n","img = np.array(d_weights.numpy())[0]\n","plot.set_title('Weight Update')\n","imgplot = plt.imshow(img, cmap='gray')\n"],"execution_count":0,"outputs":[]}]}